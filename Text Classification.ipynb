{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "01186842",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>v1</th>\n",
       "      <th>v2</th>\n",
       "      <th>Unnamed: 2</th>\n",
       "      <th>Unnamed: 3</th>\n",
       "      <th>Unnamed: 4</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     v1                                                 v2 Unnamed: 2  \\\n",
       "0   ham  Go until jurong point, crazy.. Available only ...        NaN   \n",
       "1   ham                      Ok lar... Joking wif u oni...        NaN   \n",
       "2  spam  Free entry in 2 a wkly comp to win FA Cup fina...        NaN   \n",
       "3   ham  U dun say so early hor... U c already then say...        NaN   \n",
       "4   ham  Nah I don't think he goes to usf, he lives aro...        NaN   \n",
       "\n",
       "  Unnamed: 3 Unnamed: 4  \n",
       "0        NaN        NaN  \n",
       "1        NaN        NaN  \n",
       "2        NaN        NaN  \n",
       "3        NaN        NaN  \n",
       "4        NaN        NaN  "
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# reading data\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "data = pd.read_csv('https://raw.githubusercontent.com/mohitgupta-omg/Kaggle-SMS-Spam-Collection-Dataset-/master/spam.csv', encoding='latin-1')\n",
    "\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0544e196",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  label                                               text\n",
       "0   ham  Go until jurong point, crazy.. Available only ...\n",
       "1   ham                      Ok lar... Joking wif u oni...\n",
       "2  spam  Free entry in 2 a wkly comp to win FA Cup fina...\n",
       "3   ham  U dun say so early hor... U c already then say...\n",
       "4   ham  Nah I don't think he goes to usf, he lives aro..."
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# drop unnecessary columns and rename cols\n",
    "\n",
    "data.drop(['Unnamed: 2', 'Unnamed: 3', 'Unnamed: 4'], axis=1, inplace=True)\n",
    "\n",
    "data.columns = ['label', 'text']\n",
    "\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d1123d7d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "label    0\n",
       "text     0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Exploratory Data Analysis (EDA)\n",
    "# check missing values\n",
    "\n",
    "data.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3ecc4880",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5572, 2)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check data shape\n",
    "\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "645f82d9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Axes: >"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# check target balance\n",
    "\n",
    "data['label'].value_counts(normalize = True).plot.bar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "37ea30b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading collection 'all'\n",
      "[nltk_data]    | \n",
      "[nltk_data]    | Downloading package abc to C:\\Users\\Prosenjeet\n",
      "[nltk_data]    |     Saha\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package abc is already up-to-date!\n",
      "[nltk_data]    | Downloading package alpino to C:\\Users\\Prosenjeet\n",
      "[nltk_data]    |     Saha\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package alpino is already up-to-date!\n",
      "[nltk_data]    | Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]    |     C:\\Users\\Prosenjeet\n",
      "[nltk_data]    |     Saha\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package averaged_perceptron_tagger is already up-\n",
      "[nltk_data]    |       to-date!\n",
      "[nltk_data]    | Downloading package averaged_perceptron_tagger_ru to\n",
      "[nltk_data]    |     C:\\Users\\Prosenjeet\n",
      "[nltk_data]    |     Saha\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package averaged_perceptron_tagger_ru is already\n",
      "[nltk_data]    |       up-to-date!\n",
      "[nltk_data]    | Downloading package basque_grammars to\n",
      "[nltk_data]    |     C:\\Users\\Prosenjeet\n",
      "[nltk_data]    |     Saha\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package basque_grammars is already up-to-date!\n",
      "[nltk_data]    | Downloading package bcp47 to C:\\Users\\Prosenjeet\n",
      "[nltk_data]    |     Saha\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package bcp47 is already up-to-date!\n",
      "[nltk_data]    | Downloading package biocreative_ppi to\n",
      "[nltk_data]    |     C:\\Users\\Prosenjeet\n",
      "[nltk_data]    |     Saha\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package biocreative_ppi is already up-to-date!\n",
      "[nltk_data]    | Downloading package bllip_wsj_no_aux to\n",
      "[nltk_data]    |     C:\\Users\\Prosenjeet\n",
      "[nltk_data]    |     Saha\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package bllip_wsj_no_aux is already up-to-date!\n",
      "[nltk_data]    | Downloading package book_grammars to\n",
      "[nltk_data]    |     C:\\Users\\Prosenjeet\n",
      "[nltk_data]    |     Saha\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package book_grammars is already up-to-date!\n",
      "[nltk_data]    | Downloading package brown to C:\\Users\\Prosenjeet\n",
      "[nltk_data]    |     Saha\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package brown is already up-to-date!\n",
      "[nltk_data]    | Downloading package brown_tei to C:\\Users\\Prosenjeet\n",
      "[nltk_data]    |     Saha\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package brown_tei is already up-to-date!\n",
      "[nltk_data]    | Downloading package cess_cat to C:\\Users\\Prosenjeet\n",
      "[nltk_data]    |     Saha\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package cess_cat is already up-to-date!\n",
      "[nltk_data]    | Downloading package cess_esp to C:\\Users\\Prosenjeet\n",
      "[nltk_data]    |     Saha\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package cess_esp is already up-to-date!\n",
      "[nltk_data]    | Downloading package chat80 to C:\\Users\\Prosenjeet\n",
      "[nltk_data]    |     Saha\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package chat80 is already up-to-date!\n",
      "[nltk_data]    | Downloading package city_database to\n",
      "[nltk_data]    |     C:\\Users\\Prosenjeet\n",
      "[nltk_data]    |     Saha\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package city_database is already up-to-date!\n",
      "[nltk_data]    | Downloading package cmudict to C:\\Users\\Prosenjeet\n",
      "[nltk_data]    |     Saha\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package cmudict is already up-to-date!\n",
      "[nltk_data]    | Downloading package comparative_sentences to\n",
      "[nltk_data]    |     C:\\Users\\Prosenjeet\n",
      "[nltk_data]    |     Saha\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package comparative_sentences is already up-to-\n",
      "[nltk_data]    |       date!\n",
      "[nltk_data]    | Downloading package comtrans to C:\\Users\\Prosenjeet\n",
      "[nltk_data]    |     Saha\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package comtrans is already up-to-date!\n",
      "[nltk_data]    | Downloading package conll2000 to C:\\Users\\Prosenjeet\n",
      "[nltk_data]    |     Saha\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package conll2000 is already up-to-date!\n",
      "[nltk_data]    | Downloading package conll2002 to C:\\Users\\Prosenjeet\n",
      "[nltk_data]    |     Saha\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package conll2002 is already up-to-date!\n",
      "[nltk_data]    | Downloading package conll2007 to C:\\Users\\Prosenjeet\n",
      "[nltk_data]    |     Saha\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package conll2007 is already up-to-date!\n",
      "[nltk_data]    | Downloading package crubadan to C:\\Users\\Prosenjeet\n",
      "[nltk_data]    |     Saha\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package crubadan is already up-to-date!\n",
      "[nltk_data]    | Downloading package dependency_treebank to\n",
      "[nltk_data]    |     C:\\Users\\Prosenjeet\n",
      "[nltk_data]    |     Saha\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package dependency_treebank is already up-to-date!\n",
      "[nltk_data]    | Downloading package dolch to C:\\Users\\Prosenjeet\n",
      "[nltk_data]    |     Saha\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package dolch is already up-to-date!\n",
      "[nltk_data]    | Downloading package europarl_raw to\n",
      "[nltk_data]    |     C:\\Users\\Prosenjeet\n",
      "[nltk_data]    |     Saha\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package europarl_raw is already up-to-date!\n",
      "[nltk_data]    | Downloading package extended_omw to\n",
      "[nltk_data]    |     C:\\Users\\Prosenjeet\n",
      "[nltk_data]    |     Saha\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package extended_omw is already up-to-date!\n",
      "[nltk_data]    | Downloading package floresta to C:\\Users\\Prosenjeet\n",
      "[nltk_data]    |     Saha\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package floresta is already up-to-date!\n",
      "[nltk_data]    | Downloading package framenet_v15 to\n",
      "[nltk_data]    |     C:\\Users\\Prosenjeet\n",
      "[nltk_data]    |     Saha\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package framenet_v15 is already up-to-date!\n",
      "[nltk_data]    | Downloading package framenet_v17 to\n",
      "[nltk_data]    |     C:\\Users\\Prosenjeet\n",
      "[nltk_data]    |     Saha\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package framenet_v17 is already up-to-date!\n",
      "[nltk_data]    | Downloading package gazetteers to C:\\Users\\Prosenjeet\n",
      "[nltk_data]    |     Saha\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package gazetteers is already up-to-date!\n",
      "[nltk_data]    | Downloading package genesis to C:\\Users\\Prosenjeet\n",
      "[nltk_data]    |     Saha\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package genesis is already up-to-date!\n",
      "[nltk_data]    | Downloading package gutenberg to C:\\Users\\Prosenjeet\n",
      "[nltk_data]    |     Saha\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package gutenberg is already up-to-date!\n",
      "[nltk_data]    | Downloading package ieer to C:\\Users\\Prosenjeet\n",
      "[nltk_data]    |     Saha\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package ieer is already up-to-date!\n",
      "[nltk_data]    | Downloading package inaugural to C:\\Users\\Prosenjeet\n",
      "[nltk_data]    |     Saha\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package inaugural is already up-to-date!\n",
      "[nltk_data]    | Downloading package indian to C:\\Users\\Prosenjeet\n",
      "[nltk_data]    |     Saha\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package indian is already up-to-date!\n",
      "[nltk_data]    | Downloading package jeita to C:\\Users\\Prosenjeet\n",
      "[nltk_data]    |     Saha\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package jeita is already up-to-date!\n",
      "[nltk_data]    | Downloading package kimmo to C:\\Users\\Prosenjeet\n",
      "[nltk_data]    |     Saha\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package kimmo is already up-to-date!\n",
      "[nltk_data]    | Downloading package knbc to C:\\Users\\Prosenjeet\n",
      "[nltk_data]    |     Saha\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package knbc is already up-to-date!\n",
      "[nltk_data]    | Downloading package large_grammars to\n",
      "[nltk_data]    |     C:\\Users\\Prosenjeet\n",
      "[nltk_data]    |     Saha\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package large_grammars is already up-to-date!\n",
      "[nltk_data]    | Downloading package lin_thesaurus to\n",
      "[nltk_data]    |     C:\\Users\\Prosenjeet\n",
      "[nltk_data]    |     Saha\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package lin_thesaurus is already up-to-date!\n",
      "[nltk_data]    | Downloading package mac_morpho to C:\\Users\\Prosenjeet\n",
      "[nltk_data]    |     Saha\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package mac_morpho is already up-to-date!\n",
      "[nltk_data]    | Downloading package machado to C:\\Users\\Prosenjeet\n",
      "[nltk_data]    |     Saha\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package machado is already up-to-date!\n",
      "[nltk_data]    | Downloading package masc_tagged to\n",
      "[nltk_data]    |     C:\\Users\\Prosenjeet\n",
      "[nltk_data]    |     Saha\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package masc_tagged is already up-to-date!\n",
      "[nltk_data]    | Downloading package maxent_ne_chunker to\n",
      "[nltk_data]    |     C:\\Users\\Prosenjeet\n",
      "[nltk_data]    |     Saha\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package maxent_ne_chunker is already up-to-date!\n",
      "[nltk_data]    | Downloading package maxent_treebank_pos_tagger to\n",
      "[nltk_data]    |     C:\\Users\\Prosenjeet\n",
      "[nltk_data]    |     Saha\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package maxent_treebank_pos_tagger is already up-\n",
      "[nltk_data]    |       to-date!\n",
      "[nltk_data]    | Downloading package moses_sample to\n",
      "[nltk_data]    |     C:\\Users\\Prosenjeet\n",
      "[nltk_data]    |     Saha\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package moses_sample is already up-to-date!\n",
      "[nltk_data]    | Downloading package movie_reviews to\n",
      "[nltk_data]    |     C:\\Users\\Prosenjeet\n",
      "[nltk_data]    |     Saha\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package movie_reviews is already up-to-date!\n",
      "[nltk_data]    | Downloading package mte_teip5 to C:\\Users\\Prosenjeet\n",
      "[nltk_data]    |     Saha\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package mte_teip5 is already up-to-date!\n",
      "[nltk_data]    | Downloading package mwa_ppdb to C:\\Users\\Prosenjeet\n",
      "[nltk_data]    |     Saha\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package mwa_ppdb is already up-to-date!\n",
      "[nltk_data]    | Downloading package names to C:\\Users\\Prosenjeet\n",
      "[nltk_data]    |     Saha\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package names is already up-to-date!\n",
      "[nltk_data]    | Downloading package nombank.1.0 to\n",
      "[nltk_data]    |     C:\\Users\\Prosenjeet\n",
      "[nltk_data]    |     Saha\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package nombank.1.0 is already up-to-date!\n",
      "[nltk_data]    | Downloading package nonbreaking_prefixes to\n",
      "[nltk_data]    |     C:\\Users\\Prosenjeet\n",
      "[nltk_data]    |     Saha\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package nonbreaking_prefixes is already up-to-date!\n",
      "[nltk_data]    | Downloading package nps_chat to C:\\Users\\Prosenjeet\n",
      "[nltk_data]    |     Saha\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package nps_chat is already up-to-date!\n",
      "[nltk_data]    | Downloading package omw to C:\\Users\\Prosenjeet\n",
      "[nltk_data]    |     Saha\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package omw is already up-to-date!\n",
      "[nltk_data]    | Downloading package omw-1.4 to C:\\Users\\Prosenjeet\n",
      "[nltk_data]    |     Saha\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package omw-1.4 is already up-to-date!\n",
      "[nltk_data]    | Downloading package opinion_lexicon to\n",
      "[nltk_data]    |     C:\\Users\\Prosenjeet\n",
      "[nltk_data]    |     Saha\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package opinion_lexicon is already up-to-date!\n",
      "[nltk_data]    | Downloading package panlex_swadesh to\n",
      "[nltk_data]    |     C:\\Users\\Prosenjeet\n",
      "[nltk_data]    |     Saha\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package panlex_swadesh is already up-to-date!\n",
      "[nltk_data]    | Downloading package paradigms to C:\\Users\\Prosenjeet\n",
      "[nltk_data]    |     Saha\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package paradigms is already up-to-date!\n",
      "[nltk_data]    | Downloading package pe08 to C:\\Users\\Prosenjeet\n",
      "[nltk_data]    |     Saha\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package pe08 is already up-to-date!\n",
      "[nltk_data]    | Downloading package perluniprops to\n",
      "[nltk_data]    |     C:\\Users\\Prosenjeet\n",
      "[nltk_data]    |     Saha\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package perluniprops is already up-to-date!\n",
      "[nltk_data]    | Downloading package pil to C:\\Users\\Prosenjeet\n",
      "[nltk_data]    |     Saha\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package pil is already up-to-date!\n",
      "[nltk_data]    | Downloading package pl196x to C:\\Users\\Prosenjeet\n",
      "[nltk_data]    |     Saha\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package pl196x is already up-to-date!\n",
      "[nltk_data]    | Downloading package porter_test to\n",
      "[nltk_data]    |     C:\\Users\\Prosenjeet\n",
      "[nltk_data]    |     Saha\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package porter_test is already up-to-date!\n",
      "[nltk_data]    | Downloading package ppattach to C:\\Users\\Prosenjeet\n",
      "[nltk_data]    |     Saha\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package ppattach is already up-to-date!\n",
      "[nltk_data]    | Downloading package problem_reports to\n",
      "[nltk_data]    |     C:\\Users\\Prosenjeet\n",
      "[nltk_data]    |     Saha\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package problem_reports is already up-to-date!\n",
      "[nltk_data]    | Downloading package product_reviews_1 to\n",
      "[nltk_data]    |     C:\\Users\\Prosenjeet\n",
      "[nltk_data]    |     Saha\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package product_reviews_1 is already up-to-date!\n",
      "[nltk_data]    | Downloading package product_reviews_2 to\n",
      "[nltk_data]    |     C:\\Users\\Prosenjeet\n",
      "[nltk_data]    |     Saha\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package product_reviews_2 is already up-to-date!\n",
      "[nltk_data]    | Downloading package propbank to C:\\Users\\Prosenjeet\n",
      "[nltk_data]    |     Saha\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package propbank is already up-to-date!\n",
      "[nltk_data]    | Downloading package pros_cons to C:\\Users\\Prosenjeet\n",
      "[nltk_data]    |     Saha\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package pros_cons is already up-to-date!\n",
      "[nltk_data]    | Downloading package ptb to C:\\Users\\Prosenjeet\n",
      "[nltk_data]    |     Saha\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package ptb is already up-to-date!\n",
      "[nltk_data]    | Downloading package punkt to C:\\Users\\Prosenjeet\n",
      "[nltk_data]    |     Saha\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package punkt is already up-to-date!\n",
      "[nltk_data]    | Downloading package qc to C:\\Users\\Prosenjeet\n",
      "[nltk_data]    |     Saha\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package qc is already up-to-date!\n",
      "[nltk_data]    | Downloading package reuters to C:\\Users\\Prosenjeet\n",
      "[nltk_data]    |     Saha\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package reuters is already up-to-date!\n",
      "[nltk_data]    | Downloading package rslp to C:\\Users\\Prosenjeet\n",
      "[nltk_data]    |     Saha\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package rslp is already up-to-date!\n",
      "[nltk_data]    | Downloading package rte to C:\\Users\\Prosenjeet\n",
      "[nltk_data]    |     Saha\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package rte is already up-to-date!\n",
      "[nltk_data]    | Downloading package sample_grammars to\n",
      "[nltk_data]    |     C:\\Users\\Prosenjeet\n",
      "[nltk_data]    |     Saha\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package sample_grammars is already up-to-date!\n",
      "[nltk_data]    | Downloading package semcor to C:\\Users\\Prosenjeet\n",
      "[nltk_data]    |     Saha\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package semcor is already up-to-date!\n",
      "[nltk_data]    | Downloading package senseval to C:\\Users\\Prosenjeet\n",
      "[nltk_data]    |     Saha\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package senseval is already up-to-date!\n",
      "[nltk_data]    | Downloading package sentence_polarity to\n",
      "[nltk_data]    |     C:\\Users\\Prosenjeet\n",
      "[nltk_data]    |     Saha\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package sentence_polarity is already up-to-date!\n",
      "[nltk_data]    | Downloading package sentiwordnet to\n",
      "[nltk_data]    |     C:\\Users\\Prosenjeet\n",
      "[nltk_data]    |     Saha\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package sentiwordnet is already up-to-date!\n",
      "[nltk_data]    | Downloading package shakespeare to\n",
      "[nltk_data]    |     C:\\Users\\Prosenjeet\n",
      "[nltk_data]    |     Saha\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package shakespeare is already up-to-date!\n",
      "[nltk_data]    | Downloading package sinica_treebank to\n",
      "[nltk_data]    |     C:\\Users\\Prosenjeet\n",
      "[nltk_data]    |     Saha\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package sinica_treebank is already up-to-date!\n",
      "[nltk_data]    | Downloading package smultron to C:\\Users\\Prosenjeet\n",
      "[nltk_data]    |     Saha\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package smultron is already up-to-date!\n",
      "[nltk_data]    | Downloading package snowball_data to\n",
      "[nltk_data]    |     C:\\Users\\Prosenjeet\n",
      "[nltk_data]    |     Saha\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package snowball_data is already up-to-date!\n",
      "[nltk_data]    | Downloading package spanish_grammars to\n",
      "[nltk_data]    |     C:\\Users\\Prosenjeet\n",
      "[nltk_data]    |     Saha\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package spanish_grammars is already up-to-date!\n",
      "[nltk_data]    | Downloading package state_union to\n",
      "[nltk_data]    |     C:\\Users\\Prosenjeet\n",
      "[nltk_data]    |     Saha\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package state_union is already up-to-date!\n",
      "[nltk_data]    | Downloading package stopwords to C:\\Users\\Prosenjeet\n",
      "[nltk_data]    |     Saha\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package stopwords is already up-to-date!\n",
      "[nltk_data]    | Downloading package subjectivity to\n",
      "[nltk_data]    |     C:\\Users\\Prosenjeet\n",
      "[nltk_data]    |     Saha\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package subjectivity is already up-to-date!\n",
      "[nltk_data]    | Downloading package swadesh to C:\\Users\\Prosenjeet\n",
      "[nltk_data]    |     Saha\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package swadesh is already up-to-date!\n",
      "[nltk_data]    | Downloading package switchboard to\n",
      "[nltk_data]    |     C:\\Users\\Prosenjeet\n",
      "[nltk_data]    |     Saha\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package switchboard is already up-to-date!\n",
      "[nltk_data]    | Downloading package tagsets to C:\\Users\\Prosenjeet\n",
      "[nltk_data]    |     Saha\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package tagsets is already up-to-date!\n",
      "[nltk_data]    | Downloading package timit to C:\\Users\\Prosenjeet\n",
      "[nltk_data]    |     Saha\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package timit is already up-to-date!\n",
      "[nltk_data]    | Downloading package toolbox to C:\\Users\\Prosenjeet\n",
      "[nltk_data]    |     Saha\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package toolbox is already up-to-date!\n",
      "[nltk_data]    | Downloading package treebank to C:\\Users\\Prosenjeet\n",
      "[nltk_data]    |     Saha\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package treebank is already up-to-date!\n",
      "[nltk_data]    | Downloading package twitter_samples to\n",
      "[nltk_data]    |     C:\\Users\\Prosenjeet\n",
      "[nltk_data]    |     Saha\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package twitter_samples is already up-to-date!\n",
      "[nltk_data]    | Downloading package udhr to C:\\Users\\Prosenjeet\n",
      "[nltk_data]    |     Saha\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package udhr is already up-to-date!\n",
      "[nltk_data]    | Downloading package udhr2 to C:\\Users\\Prosenjeet\n",
      "[nltk_data]    |     Saha\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package udhr2 is already up-to-date!\n",
      "[nltk_data]    | Downloading package unicode_samples to\n",
      "[nltk_data]    |     C:\\Users\\Prosenjeet\n",
      "[nltk_data]    |     Saha\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package unicode_samples is already up-to-date!\n",
      "[nltk_data]    | Downloading package universal_tagset to\n",
      "[nltk_data]    |     C:\\Users\\Prosenjeet\n",
      "[nltk_data]    |     Saha\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package universal_tagset is already up-to-date!\n",
      "[nltk_data]    | Downloading package universal_treebanks_v20 to\n",
      "[nltk_data]    |     C:\\Users\\Prosenjeet\n",
      "[nltk_data]    |     Saha\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package universal_treebanks_v20 is already up-to-\n",
      "[nltk_data]    |       date!\n",
      "[nltk_data]    | Downloading package vader_lexicon to\n",
      "[nltk_data]    |     C:\\Users\\Prosenjeet\n",
      "[nltk_data]    |     Saha\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package vader_lexicon is already up-to-date!\n",
      "[nltk_data]    | Downloading package verbnet to C:\\Users\\Prosenjeet\n",
      "[nltk_data]    |     Saha\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package verbnet is already up-to-date!\n",
      "[nltk_data]    | Downloading package verbnet3 to C:\\Users\\Prosenjeet\n",
      "[nltk_data]    |     Saha\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package verbnet3 is already up-to-date!\n",
      "[nltk_data]    | Downloading package webtext to C:\\Users\\Prosenjeet\n",
      "[nltk_data]    |     Saha\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package webtext is already up-to-date!\n",
      "[nltk_data]    | Downloading package wmt15_eval to C:\\Users\\Prosenjeet\n",
      "[nltk_data]    |     Saha\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package wmt15_eval is already up-to-date!\n",
      "[nltk_data]    | Downloading package word2vec_sample to\n",
      "[nltk_data]    |     C:\\Users\\Prosenjeet\n",
      "[nltk_data]    |     Saha\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package word2vec_sample is already up-to-date!\n",
      "[nltk_data]    | Downloading package wordnet to C:\\Users\\Prosenjeet\n",
      "[nltk_data]    |     Saha\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package wordnet is already up-to-date!\n",
      "[nltk_data]    | Downloading package wordnet2021 to\n",
      "[nltk_data]    |     C:\\Users\\Prosenjeet\n",
      "[nltk_data]    |     Saha\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package wordnet2021 is already up-to-date!\n",
      "[nltk_data]    | Downloading package wordnet2022 to\n",
      "[nltk_data]    |     C:\\Users\\Prosenjeet\n",
      "[nltk_data]    |     Saha\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package wordnet2022 is already up-to-date!\n",
      "[nltk_data]    | Downloading package wordnet31 to C:\\Users\\Prosenjeet\n",
      "[nltk_data]    |     Saha\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package wordnet31 is already up-to-date!\n",
      "[nltk_data]    | Downloading package wordnet_ic to C:\\Users\\Prosenjeet\n",
      "[nltk_data]    |     Saha\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package wordnet_ic is already up-to-date!\n",
      "[nltk_data]    | Downloading package words to C:\\Users\\Prosenjeet\n",
      "[nltk_data]    |     Saha\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package words is already up-to-date!\n",
      "[nltk_data]    | Downloading package ycoe to C:\\Users\\Prosenjeet\n",
      "[nltk_data]    |     Saha\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package ycoe is already up-to-date!\n",
      "[nltk_data]    | \n",
      "[nltk_data]  Done downloading collection all\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>go jurong point crazy available bugis n great ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>ok lar joking wif u oni</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spam</td>\n",
       "      <td>free entry wkly comp win fa cup final tkts st ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>u dun say early hor u c already say</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>nah think go usf life around though</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  label                                               text\n",
       "0   ham  go jurong point crazy available bugis n great ...\n",
       "1   ham                            ok lar joking wif u oni\n",
       "2  spam  free entry wkly comp win fa cup final tkts st ...\n",
       "3   ham                u dun say early hor u c already say\n",
       "4   ham                nah think go usf life around though"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Text Preprocessing\n",
    "#This is where all text cleaning takes place. It’s a loop that iterates through all 5,572 documents and does the following:\n",
    "\n",
    "#Remove all special characters\n",
    "#Lowercase all the words\n",
    "#Tokenize\n",
    "#Remove stopwords\n",
    "#Lemmatize\n",
    "# text preprocessing\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# download nltk\n",
    "\n",
    "import nltk\n",
    "\n",
    "nltk.download('all')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# create a list text\n",
    "\n",
    "text = list(data['text'])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# preprocessing loop\n",
    "\n",
    "import re\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "corpus = []\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "for i in range(len(text)):\n",
    "\n",
    "    r = re.sub('[^a-zA-Z]', ' ', text[i])\n",
    "\n",
    "    r = r.lower()\n",
    "\n",
    "    r = r.split()\n",
    "\n",
    "    r = [word for word in r if word not in stopwords.words('english')]\n",
    "\n",
    "    r = [lemmatizer.lemmatize(word) for word in r]\n",
    "\n",
    "    r = ' '.join(r)\n",
    "\n",
    "    corpus.append(r)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#assign corpus to data['text']\n",
    "\n",
    "data['text'] = corpus\n",
    "\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4c644cd6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Data : (3733,)\n",
      "Testing Data :  (1839,)\n"
     ]
    }
   ],
   "source": [
    "# Create Feature and Label sets\n",
    "\n",
    "X = data['text']\n",
    "\n",
    "y = data['label']\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# train test split (66% train - 33% test)\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=123)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print('Training Data :', X_train.shape)\n",
    "\n",
    "print('Testing Data : ', X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "70af760a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3733, 5685)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Feature Extraction\n",
    "# Train Bag of Words model\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "cv = CountVectorizer()\n",
    "\n",
    "X_train_cv = cv.fit_transform(X_train)\n",
    "\n",
    "X_train_cv.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e5a89b13",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['ham', 'spam', 'ham', ..., 'ham', 'ham', 'spam'], dtype=object)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Model Training and Evaluation\n",
    "# Training Logistic Regression model\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "lr = LogisticRegression()\n",
    "\n",
    "lr.fit(X_train_cv, y_train)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# transform X_test using CV\n",
    "\n",
    "X_test_cv = cv.transform(X_test)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# generate predictions\n",
    "\n",
    "predictions = lr.predict(X_test_cv)\n",
    "\n",
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "438a900a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ham</th>\n",
       "      <th>spam</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>ham</th>\n",
       "      <td>1600</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>spam</th>\n",
       "      <td>31</td>\n",
       "      <td>206</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       ham  spam\n",
       "ham   1600     2\n",
       "spam    31   206"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# confusion matrix\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn import metrics\n",
    "\n",
    "df = pd.DataFrame(metrics.confusion_matrix(y_test,predictions), index=['ham','spam'], columns=['ham','spam'])\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1eaf61f0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ham</th>\n",
       "      <th>spam</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>ham</th>\n",
       "      <td>1600</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>spam</th>\n",
       "      <td>31</td>\n",
       "      <td>206</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       ham  spam\n",
       "ham   1600     2\n",
       "spam    31   206"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# confusion matrix\n",
    "import pandas as pd\n",
    "df = pd.DataFrame(metrics.confusion_matrix(y_test,predictions), index=['ham','spam'], columns=['ham','spam'])\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e1a8361",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
